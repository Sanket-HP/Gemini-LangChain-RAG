{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df5d726-4dd0-4158-bee0-f0abaa1bd6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install langchain_experimental langchain_core\n",
    "!pip -q install google-generativeai==0.3.1\n",
    "!pip -q install google-ai-generativelanguage==0.4.0\n",
    "!pip -q install langchain-google-genai\n",
    "!pip -q install wikipedia\n",
    "!pip -q install docarray\n",
    "!pip -q install --upgrade protobuf google.protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c42420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before proceeding with the next cells restart the kernel by clicking the refresh icon on the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931ae5e6-7580-4d67-b7fd-3a875b8cf649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from google.protobuf.empty_pb2 import Empty\n",
    "\n",
    "key_name = !gcloud services api-keys list --filter=\"gemini-api-key\" --format=\"value(name)\"\n",
    "key_name = key_name[0]\n",
    "\n",
    "api_key = !gcloud services api-keys get-key-string $key_name --location=\"us-central1\" --format=\"value(keyString)\"\n",
    "api_key = api_key[0]\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0538d742-6944-4041-9dde-073385a87b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/chat-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 Chat (Legacy)',\n",
       "       description='A legacy text-only model optimized for chat conversations',\n",
       "       input_token_limit=4096,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "       temperature=0.25,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/text-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 (Legacy)',\n",
       "       description='A legacy model that understands text and generates text as an output',\n",
       "       input_token_limit=8196,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "       temperature=0.7,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
       "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
       "                    'model that supports tuning.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Latest',\n",
       "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
       "                    'model.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-vision-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-1.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 001',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash Latest',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro 001',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro Latest',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-pro-vision',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [m for m in genai.list_models()]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36455709-7797-4a28-a9a9-89a038e96b26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Gemini directly with Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d08290d-120a-489e-a4e3-46bfbd619bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am Gemini, a multi-modal AI model, developed by Google. I am designed to provide information and assist users with a wide range of topics and tasks. Here are some of my capabilities:\n",
       "\n",
       "**Information Retrieval and Generation:**\n",
       "* Answer questions on various topics, including science, history, current events, and more.\n",
       "* Generate text in different styles and tones, such as summaries, articles, stories, and poems.\n",
       "* Translate text between over 100 languages.\n",
       "\n",
       "**Conversational Skills:**\n",
       "* Engage in natural language conversations and understand the context and intent of user queries.\n",
       "* Provide personalized responses based on the user's preferences and history.\n",
       "* Generate creative responses, such as jokes, puns, and riddles.\n",
       "\n",
       "**Task Assistance:**\n",
       "* Set alarms, timers, and reminders.\n",
       "* Perform mathematical calculations and unit conversions.\n",
       "* Provide weather forecasts and news updates.\n",
       "* Help with scheduling appointments and making reservations.\n",
       "\n",
       "**Knowledge and Understanding:**\n",
       "* Possess a vast knowledge base across a wide range of domains, including:\n",
       "    * Science and technology\n",
       "    * Arts and culture\n",
       "    * History and geography\n",
       "    * Business and finance\n",
       "* Continuously learn and update my knowledge through ongoing training.\n",
       "\n",
       "**Additional Features:**\n",
       "* Access and process information from the internet and various databases.\n",
       "* Integrate with external applications and services through APIs.\n",
       "* Generate images, music, and other creative content.\n",
       "\n",
       "Please note that while I am capable of performing many tasks and providing information, I am still under development and my abilities are constantly evolving. If you have any specific questions or requests, feel free to ask, and I will do my best to assist you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate text\n",
    "prompt = 'Who are you and what can you do?'\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "Markdown(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214aef1-1aad-4469-a216-9d4a9b2cb278",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Gemini with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7f1ae-13f2-4a28-bd09-237eacea1106",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef52da8e-0cee-409b-8a37-5c2ddca42aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LLM stands for Large Language Model. LLMs are a type of artificial intelligence (AI) that are trained on massive datasets of text and code. They are able to understand and generate human-like text, translate languages, write different kinds of creative content, and even write different kinds of code.\n",
       "\n",
       "LLMs are used in a wide variety of applications, including:\n",
       "\n",
       "* **Natural language processing (NLP)**: LLMs can be used to perform a variety of NLP tasks, such as text classification, sentiment analysis, machine translation, and question answering.\n",
       "* **Machine learning (ML)**: LLMs can be used to train ML models on text data. This can be useful for tasks such as image classification, speech recognition, and recommendation systems.\n",
       "* **Creative content generation**: LLMs can be used to generate creative content, such as stories, poems, and songs.\n",
       "* **Code generation**: LLMs can be used to generate code in a variety of programming languages. This can be useful for tasks such as software development and bug fixing.\n",
       "\n",
       "LLMs are still under development, but they have the potential to revolutionize a wide range of industries. They are already being used to create new products and services, and they are likely to play an even bigger role in our lives in the years to come."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)\n",
    "\n",
    "\n",
    "result = llm.invoke(\"What is a LLM?\")\n",
    "\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202f5831-31e8-429a-9a79-e26fb096f9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words flow like a stream\n",
      "AI's mimicry, it gleams\n",
      "\n",
      "---\n",
      "Language's new dream\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Write a haiku about LLMs.\"):\n",
    "    print(chunk.content)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1410317-db72-4c2a-99dc-279938a07074",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Multi Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29cd459-e106-4407-8561-253d8b55ccf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e53795e-8b12-4166-82a9-e242f6cf80c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38cc1c15-a8b9-48fc-a7cc-3cb9666fb822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "677fb13f-b6b1-407e-a4e0-16d16e31f636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the machine learning algorithm get lost in the forest?\\n\\nBecause it overfit the training data.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"machine learning\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0802e31-78d7-42fa-a830-7a5daeed7872",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A more complicated Chain - RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3086e684-8311-484a-84c4-d58a415c3c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9e30de0-7dda-4997-9c54-c01d028e1c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aba2d34a-2a31-48c5-9ce1-4940a731b340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39116cea-1261-4eeb-a69a-9acc5b0104b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': 'Machine learning', 'summary': \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}, page_content='Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field\\'s methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\\n\\n\\n== History ==\\n\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\\nAlthough the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb\\'s model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\\nBy the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\\nModern-day machine learning has two objectives.  One is to classify data based on model')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "# use Wikipedia loader to create some docs to use..\n",
    "docs = WikipediaLoader(query=\"Machine Learning\", load_max_docs=10).load()\n",
    "docs += WikipediaLoader(query=\"Deep Learning\", load_max_docs=10).load() \n",
    "docs += WikipediaLoader(query=\"Neural Networks\", load_max_docs=10).load() \n",
    "\n",
    "# Take a look at a single document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6bfe25-419d-4caa-80d0-4878fb67cdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 11:21:35.923191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-15 11:21:36.245646: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-15 11:21:36.249293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-15 11:21:36.754615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-15 11:21:41.626153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings # passing in the model to embed documents..\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2451a12-1e00-414a-a187-ab9675edb752",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Machine learning', 'summary': \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}, page_content='Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field\\'s methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\\n\\n\\n== History ==\\n\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\\nAlthough the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb\\'s model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\\nBy the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\\nModern-day machine learning has two objectives.  One is to classify data based on model'),\n",
       " Document(metadata={'title': 'Adversarial machine learning', 'summary': 'Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.\\nMost machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\\nMost common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.', 'source': 'https://en.wikipedia.org/wiki/Adversarial_machine_learning'}, page_content='Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.\\nMost machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\\nMost common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.\\n\\n\\n== History ==\\nAt the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine-learning spam filter could be used to defeat another machine-learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam.\\nIn 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple \"evasion attacks\" as spammers inserted \"good words\" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within \"image spam\" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published \"Can Machine Learning Be Secure?\", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012â€“2013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations.\\nRecently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noise. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brain\\'s Nicholas Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state-of-the-art approaches.\\nWhile adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks.\\n\\n\\n=== Examples ===\\nExamples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of \"bad\" words or the insertion of \"good\" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users\\' template galleries that adapt to updated traits over time.\\nResearchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy turtle w'),\n",
       " Document(metadata={'title': 'Neural network (machine learning)', 'summary': 'In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in a brain. These are connected by edges, which model the synapses in a brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.\\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Neural_network_(machine_learning)'}, page_content='In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in a brain. These are connected by edges, which model the synapses in a brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.\\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n\\n== Training ==\\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\\'s parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\\n\\n\\n== History ==\\n\\nHistorically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\\nThe simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\\nWarren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.\\nIn the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a \\'typical\\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\\'s \"unorganized machines\". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines, then called \"calculators\". Other neural network computational machines were created by Rochester, Holland, Habit, and Duda in 1956. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first implemented artifi'),\n",
       " Document(metadata={'title': 'Neural network (machine learning)', 'summary': 'In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in a brain. These are connected by edges, which model the synapses in a brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.\\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Neural_network_(machine_learning)'}, page_content='In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in a brain. These are connected by edges, which model the synapses in a brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.\\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n\\n== Training ==\\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\\'s parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\\n\\n\\n== History ==\\n\\nHistorically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\\nThe simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\\nWarren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.\\nIn the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a \\'typical\\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\\'s \"unorganized machines\". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines, then called \"calculators\". Other neural network computational machines were created by Rochester, Holland, Habit, and Duda in 1956. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first implemented artifi')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what is machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8ec6628-e399-4d0a-bc71-b74eac511648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Graph neural network', 'summary': 'A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.\\n\\nIn the more general subject of \"geometric deep learning\", certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer, in the context of computer vision, can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer, in natural language processing, can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text.\\nThe key design element of GNNs is the use of pairwise message passing, such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception, several different GNN architectures have been proposed, which implement different flavors of message passing,  started by recursive or convolutional constructive approaches. As of 2022, whether it is possible to define GNN architectures \"going beyond\" message passing, or if every GNN can be built on message passing over suitably defined graphs, is an open research question.\\nRelevant application domains for GNNs include Natural Language Processing, social networks, citation networks, molecular biology, chemistry, physics and NP-hard combinatorial optimization problems.\\nSeveral open source libraries implementing graph neural networks are available, such as PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).', 'source': 'https://en.wikipedia.org/wiki/Graph_neural_network'}, page_content='A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.\\n\\nIn the more general subject of \"geometric deep learning\", certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer, in the context of computer vision, can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer, in natural language processing, can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text.\\nThe key design element of GNNs is the use of pairwise message passing, such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception, several different GNN architectures have been proposed, which implement different flavors of message passing,  started by recursive or convolutional constructive approaches. As of 2022, whether it is possible to define GNN architectures \"going beyond\" message passing, or if every GNN can be built on message passing over suitably defined graphs, is an open research question.\\nRelevant application domains for GNNs include Natural Language Processing, social networks, citation networks, molecular biology, chemistry, physics and NP-hard combinatorial optimization problems.\\nSeveral open source libraries implementing graph neural networks are available, such as PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).\\n\\n\\n== Architecture ==\\nThe architecture of a generic GNN implements the following fundamental layers:\\n\\nPermutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature, permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively, in a message passing layer, nodes update their representations by aggregating the messages received from their immediate neighbours. As such, each message passing layer increases the receptive field of the GNN by one hop.\\nLocal pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN, in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling, top-k pooling, and self-attention pooling.\\nGlobal pooling: a global pooling layer, also known as readout layer, provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant, such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum, mean or maximum.\\nIt has been demonstrated that GNNs cannot be more expressive than the Weisfeilerâ€“Leman Graph Isomorphism Test. In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022, whether or not future architectures will overcome the message passing primitive is an open research question.\\n\\n\\n== Message passing layers ==\\n\\nMessage passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally, they can be expressed as message passing neural networks (MPNNs).\\nLet \\n  \\n    \\n      \\n        G\\n        =\\n        (\\n        V\\n        ,\\n        E\\n        )\\n      \\n    \\n    {\\\\displaystyle G=(V,E)}\\n  \\n be a graph, where \\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n  \\n is the node set and \\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n  \\n is the edge set. Let \\n  \\n    \\n      \\n        \\n          N\\n          '),\n",
       " Document(metadata={'title': 'Comparison of deep learning software', 'summary': 'The following table compares notable software frameworks, libraries and computer programs for deep learning.', 'source': 'https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software'}, page_content='The following table compares notable software frameworks, libraries and computer programs for deep learning.\\n\\n\\n== Deep-learning software by name ==\\n\\n\\n== Comparison of compatibility of machine learning models ==\\n\\n\\n== See also ==\\nComparison of numerical-analysis software\\nComparison of statistical packages\\nComparison of cognitive architectures\\nList of datasets for machine-learning research\\nList of numerical-analysis software\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Mamba (deep learning architecture)', 'summary': 'Mamba is a deep learning architecture focused on sequence modeling. It was developed by researchers from Carnegie Mellon University and Princeton University to address some limitations of transformer models, especially in processing long sequences. It is based on the Structured State Space sequence (S4) model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Mamba_(deep_learning_architecture)'}, page_content='Mamba is a deep learning architecture focused on sequence modeling. It was developed by researchers from Carnegie Mellon University and Princeton University to address some limitations of transformer models, especially in processing long sequences. It is based on the Structured State Space sequence (S4) model.\\n\\n\\n== Architecture ==\\nTo enable handling long data sequences, Mamba incorporates the Structured State Space sequence model (S4). S4 can effectively and efficiently model long dependencies by combining the strengths of continuous-time, recurrent, and convolutional models, enabling it to handle irregularly sampled data, have unbounded context, and remain computationally efficient both during training and testing.\\nMamba, building on the S4 model, introduces significant enhancements, particularly in its treatment of time-variant operations. Central to its design is a unique selection mechanism that adapts structured state space model (SSM) parameters based on the input. This enables Mamba to selectively focus on relevant information within sequences, effectively filtering out less pertinent data. The model transitions from a time-invariant to a time-varying framework, which impacts both the computation and efficiency of the system.\\nTo address the computational challenges introduced by this time-variance, Mamba employs a hardware-aware algorithm. This algorithm enables efficient computation on modern hardware, like GPUs, by using kernel fusion, parallel scan, and recomputation. The implementation avoids materializing expanded states in memory-intensive layers, thereby optimizing performance and memory usage. The result is an architecture that is significantly more efficient in processing long sequences compared to previous methods.\\nAdditionally, Mamba simplifies its architecture by integrating the SSM design with MLP blocks, resulting in a homogeneous and streamlined structure, furthering the model\\'s capability for general sequence modeling across various data types, including language, audio, and genomics, while maintaining efficiency in both training and inference.\\n\\n\\n=== Key components ===\\nSelective-State-Spaces (SSM): The core of Mamba, SSMs are recurrent models that selectively process information based on the current input. This allows them to focus on relevant information and discard irrelevant data, potentially leading to efficient processing.\\nSimplified Architecture: Mamba replaces the complex attention and MLP blocks of Transformers with a single, unified SSM block. This aims to reduce computational complexity and improve inference speed.\\nHardware-Aware Parallelism: Mamba utilizes a recurrent mode with a parallel algorithm specifically designed for hardware efficiency, potentially further enhancing its performance.\\n\\n\\n=== Comparison to Transformers ===\\n\\n\\n== Variants ==\\n\\n\\n=== Token-free language models: MambaByte ===\\n\\nOperating on byte-sized tokens, transformers scale poorly as every token must \"attend\" to every other token leading to O(n2) scaling laws, as a result, Transformers opt to use subword tokenization to reduce the number of tokens in text, however, this leads to very large vocabulary tables and word embeddings.\\nThis research investigates a novel approach to language modeling, MambaByte, which departs from the standard token-based methods. Unlike traditional models that rely on breaking text into discrete units, MambaByte directly processes raw byte sequences.  This eliminates the need for tokenization, potentially offering several advantages:\\n\\nLanguage Independence: Tokenization often relies on language-specific rules and vocabulary, limiting applicability across diverse languages. MambaByte\\'s byte-level representation allows it to handle different languages without language-specific adaptations.\\nRemoves the bias of subword tokenisation: where common subwords are overrepresented and rare or new words are underrepresented or split into less meaningful units. This can affect the model\\'s understanding and genera'),\n",
       " Document(metadata={'title': 'Physics-informed neural networks', 'summary': 'Physics-informed neural networks (PINNs), also referred to as Theory-Trained Neural Networks (TTNs), are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs).They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness, rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the correctness of the function approximation. This way, embedding this prior information into a neural network results in enhancing the information content of the available data, facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Physics-informed_neural_networks'}, page_content='Physics-informed neural networks (PINNs), also referred to as Theory-Trained Neural Networks (TTNs), are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs).They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness, rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the correctness of the function approximation. This way, embedding this prior information into a neural network results in enhancing the information content of the available data, facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples.\\n\\n\\n== Function approximation ==\\nMost of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example, the Navierâ€“Stokes equations are a set of partial differential equations derived from the conservation laws (i.e., conservation of mass, momentum, and energy) that govern fluid mechanics. The solution of the Navierâ€“Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However, these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences, finite elements and finite volumes). In this setting, these governing equations must be solved while accounting for prior assumptions, linearization, and adequate time and space discretization.\\nRecently, solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML), leveraging the universal approximation theorem and high expressivity of neural networks. In general, deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However, such networks do not consider the physical characteristics underlying the problem, and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information, the solution is not unique and may lose physical correctness. On the other hand, physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely, PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion, a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially, an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore, with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete), PINN may be used for finding an optimal solution with high fidelity.\\nPINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g., CFD for fluid dynamics), and new data-driven approaches for model inversion and system identification. Notably, the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition, they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations, a new class of diff')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what is gemini pro?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773eb751-451d-429a-ae3e-75287baea2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question a a full sentence, based only on the following context:\n",
    "{context}\n",
    "\n",
    "Return you answer in three back ticks\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "469a5efb-90ac-4b51-b323-cd5dceb95984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac53ade5-9662-4f5a-a033-99622809163f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Boosting (machine learning)', 'summary': 'In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, variance. It is used in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones.\\nThe concept of boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\\nRobert Schapire answered the question posed by Kearns and Valiant in the affirmative in a paper published in 1990.This has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting. \\nWhen first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm [â€¦] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.', 'source': 'https://en.wikipedia.org/wiki/Boosting_(machine_learning)'}, page_content='In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, variance. It is used in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones.\\nThe concept of boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\\nRobert Schapire answered the question posed by Kearns and Valiant in the affirmative in a paper published in 1990.This has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting. \\nWhen first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm [â€¦] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire\\'s arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\\n\\n\\n== Boosting algorithms ==\\nWhile boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners\\' accuracy.  After a weak learner is added, the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.\\n\\nThere are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation), and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious GÃ¶del Prize.\\nOnly algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called \"leveraging algorithms\", although they are also sometimes incorrectly called boosting algorithms.\\nThe main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses. There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others. Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function.\\n\\n\\n== Object categorization in computer vision ==\\n\\nGiven images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.\\n\\n\\n=== Probl'),\n",
       " Document(metadata={'title': 'Feedforward neural network', 'summary': 'A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one directionâ€”forwardâ€”from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops, in contrast to recurrent neural networks, which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the \"vanilla\" neural networks.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Feedforward_neural_network'}, page_content='A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one directionâ€”forwardâ€”from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops, in contrast to recurrent neural networks, which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the \"vanilla\" neural networks.\\n\\n\\n== Timeline ==\\nIn 1958, a layered network of perceptrons, consisting of an input layer, a hidden layer with randomized weights that did not learn, and an output layer with learning connections, was introduced already by Frank Rosenblatt in his book Perceptron. This extreme learning machine was not yet a deep learning network.\\nIn 1965, the first  deep-learning feedforward network, not yet using stochastic gradient descent, was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa, at the time called the Group Method of Data Handling.\\nIn 1967, a deep-learning network, using stochastic gradient descent for the first time, was able to classify non-linearily separable pattern classes, as reported Shun\\'ichi Amari. Amari\\'s student Saito conducted the computer experiments, using a five-layered feedforward network with two learning layers.\\nIn 1970, modern backpropagation method, an efficient application of a chain-rule-based supervised learning, was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. \"back-propagating errors\") itself has been used by Rosenblatt himself, but he did not know how to implement it, although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.\\nIn 1982, backpropagation was applied in the way that has become standard, for the first time by Paul Werbos.\\nIn 1985, an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.\\nIn 1987, using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network, Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth, using small batches of random input/output samples. He, however, concluded that on hardware (sub-megaflop computers) available at the time it was impractical, and proposed using fixed random early layers as an input hash for a single modifiable layer.\\nIn 1990s, an (much simpler) alternative to using neural networks, although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification, they were able to efficiently perform a non-linear classification using what is called the kernel trick, using high-dimensional feature spaces.\\nIn 2003, interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.\\nIn 2017, modern transformer architectures were introduced in the Attention Is All You Need paper.\\n\\n\\n== Mathematical foundations ==\\n\\n\\n=== Activation function ===\\nThe two historically common activation functions are both sigmoids, and are described by\\n\\n  \\n    \\n      \\n        y\\n        (\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        tanh\\n        \\u2061\\n        (\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        )\\n         \\n         \\n        \\n          \\n            and\\n          \\n        \\n         \\n         \\n        y\\n        (\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        (\\n        1\\n        +\\n        \\n          e\\n          \\n            âˆ’\\n            \\n              v\\n     '),\n",
       " Document(metadata={'title': 'Attention (machine learning)', 'summary': 'The machine learning-based attention method simulates how human attention works by assigning varying levels of importance to different words in a sentence.  It assigns importance to each word by calculating \"soft\" weights for the word\\'s numerical representation, known as its embedding, within a specific section of the sentence called the context window to determine its importance. The calculation of these weights can occur simultaneously in models called transformers, or one by one in models known as recurrent neural networks. Unlike \"hard\" weights, which are predetermined and fixed during training, \"soft\" weights can adapt and change with each use of the model.\\nAttention was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows the calculation of the hidden representation of a token equal access to any part of a sentence directly, rather than only through the previous hidden state.  \\nEarlier uses attached this mechanism to a serial recurrent neural network\\'s language translation system (below), but later uses in transformers\\' large language models removed the recurrent neural network and relied heavily on the faster parallel attention scheme.', 'source': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)'}, page_content='The machine learning-based attention method simulates how human attention works by assigning varying levels of importance to different words in a sentence.  It assigns importance to each word by calculating \"soft\" weights for the word\\'s numerical representation, known as its embedding, within a specific section of the sentence called the context window to determine its importance. The calculation of these weights can occur simultaneously in models called transformers, or one by one in models known as recurrent neural networks. Unlike \"hard\" weights, which are predetermined and fixed during training, \"soft\" weights can adapt and change with each use of the model.\\nAttention was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows the calculation of the hidden representation of a token equal access to any part of a sentence directly, rather than only through the previous hidden state.  \\nEarlier uses attached this mechanism to a serial recurrent neural network\\'s language translation system (below), but later uses in transformers\\' large language models removed the recurrent neural network and relied heavily on the faster parallel attention scheme.\\n\\n\\n== Predecessors ==\\nPredecessors of the mechanism were used in recurrent neural networks which, however, calculated \"soft\" weights sequentially and, at each step, considered the current word and other words within the context window. They were known as multiplicative modules, sigma pi units, and hyper-networks. They have been used in long short-term memory (LSTM) networks, multi-sensory data processing (sound, images, video, and text) in perceivers, fast weight controller\\'s memory, reasoning tasks in differentiable neural computers, and neural Turing machines.\\n\\n\\n== Core calculations ==\\nThe attention network was designed to identify the highest correlations amongst words within a sentence, assuming that it has learned those patterns from the training corpus.  This correlation is captured in neuronal weights through backpropagation, either from self-supervised pretraining or supervised fine-tuning. \\nThe example below (an encoder-only QKV variant of an attention network) shows how correlations are identified once a network has been trained and has the right weights.  When looking at the word \"that\" in the sentence \"see that girl run\", the network should be able to identify \"girl\" as a highly correlated word.  For simplicity this example focuses on the word \"that\", but in reality all words receive this treatment in parallel and the resulting soft-weights and context vectors are stacked into matrices for further task-specific use.\\n\\nThe query vector is compared (via dot product) with each word in the keys. This helps the model discover the most relevant word for the query word. In this case \"girl\" was determined to be the most relevant word for \"that\". The result (size 4 in this case) is run through the softmax function, producing a vector of size 4 with probabilities summing to 1. Multiplying this against the value matrix effectively amplifies the signal for the most important words in the sentence and diminishes the signal for less important words.\\nThe structure of the input data is captured in the Wq and Wk weights, and the Wv weights express that structure in terms of more meaningful features for the task being trained for.  For this reason, the attention head components are called Query (Wq), Key (Wk), and Value (Wv)â€”a loose and possibly misleading analogy with relational database systems.\\nNote that the context vector for \"that\" does not rely on context vectors for the other words; therefore the context vectors of all words can be calculated using the whole matrix X, which includes all the word embeddings, instead of a single word\\'s embedding vect'),\n",
       " Document(metadata={'title': 'Rectifier (neural networks)', 'summary': 'In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument:\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          x\\n          \\n            +\\n          \\n        \\n        =\\n        max\\n        (\\n        0\\n        ,\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              x\\n              +\\n              \\n                |\\n              \\n              x\\n              \\n                |\\n              \\n            \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            {\\n            \\n              \\n                \\n                  x\\n                \\n                \\n                  \\n                    if \\n                  \\n                  x\\n                  >\\n                  0\\n                  ,\\n                \\n              \\n              \\n                \\n                  0\\n                \\n                \\n                  \\n                    otherwise\\n                  \\n                  ,\\n                \\n              \\n            \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)=x^{+}=\\\\max(0,x)={\\\\frac {x+|x|}{2}}={\\\\begin{cases}x&{\\\\text{if }}x>0,\\\\\\\\0&{\\\\text{otherwise}},\\\\end{cases}}}\\n  \\n\\nwhere \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks.\\nRectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience.', 'source': 'https://en.wikipedia.org/wiki/Rectifier_(neural_networks)'}, page_content='In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument:\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          x\\n          \\n            +\\n          \\n        \\n        =\\n        max\\n        (\\n        0\\n        ,\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              x\\n              +\\n              \\n                |\\n              \\n              x\\n              \\n                |\\n              \\n            \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            {\\n            \\n              \\n                \\n                  x\\n                \\n                \\n                  \\n                    if \\n                  \\n                  x\\n                  >\\n                  0\\n                  ,\\n                \\n              \\n              \\n                \\n                  0\\n                \\n                \\n                  \\n                    otherwise\\n                  \\n                  ,\\n                \\n              \\n            \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)=x^{+}=\\\\max(0,x)={\\\\frac {x+|x|}{2}}={\\\\begin{cases}x&{\\\\text{if }}x>0,\\\\\\\\0&{\\\\text{otherwise}},\\\\end{cases}}}\\n  \\n\\nwhere \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks.\\nRectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience.\\n\\n\\n== Advantages ==\\nSparse activation: For example, in a randomly initialized network, only about 50% of hidden units are activated (have a non-zero output).\\nBetter gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions.\\nEfficient computation: Only comparison, addition and multiplication.\\nScale-invariant (homogeneous): \\n  \\n    \\n      \\n        max\\n        (\\n        0\\n        ,\\n        a\\n        x\\n        )\\n        =\\n        a\\n        max\\n        (\\n        0\\n        ,\\n        x\\n        )\\n        \\n           for \\n        \\n        a\\n        â‰¥\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\max(0,ax)=a\\\\max(0,x){\\\\text{ for }}a\\\\geq 0}\\n  \\n.\\nRectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid, which was trained in a supervised way to learn several computer vision tasks. In 2011, the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units, compared to sigmoid function or similar activation functions, allow faster and effective training of deep neural architectures on large and complex datasets.\\n\\n\\n== Potential problems ==\\nNon-differentiable at zero; however, it is differentiable anywhere else, and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1.\\nNot zero-centered: ReLU outputs are always non-negative. This can make it harder for the network to learn during backpropagation because gradient updates tend to push weights in one direction (positive or negative). Batch normalization can help addres')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is a graident boosted tree?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84ec2b44-1798-4695-936c-7026b00117d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3b7b8f4-5426-4e87-8ac4-b58159cfa4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions.```'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7751c12f-922e-4d75-8ce4-8533dc0c024b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```\\n2017\\n```'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"When was the transformer invented?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd5b2c-81e6-4961-a12d-6e886fb52b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-16.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-16:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
